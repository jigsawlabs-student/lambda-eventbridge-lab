{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef4171c1-4bb2-4301-9fff-60b2313fea02",
   "metadata": {},
   "source": [
    "# S3 Lambda Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee423d8-0707-4aa9-bb18-29a6f90ffabb",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b219dcf5-e7cf-47f3-acc0-f16e0f0662bc",
   "metadata": {},
   "source": [
    "In this lesson, we'll practice creating buckets and objects, and then reading data from those buckets and objects.  Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3e90aa-1ed1-402a-91b6-aa5fff464a91",
   "metadata": {},
   "source": [
    "### Migrate to lambda\n",
    "\n",
    "The first thing we should do is look at the code that we will be moving to s3.  \n",
    "\n",
    "* Notice that in the `index.py` file we wrapped our code in a function called `lambda_handler`.  This is the function that will ultimately be executed when we move this to lambda.  \n",
    "* Remember that the way that this code worked is that we created requested some json from the api, then stored this data in a file, and then saved this file on a directory in our computer.  When we copy this code into our lambda function on AWS, we will have to create file on the computer that is running our lambda function.  AWS allows us to do this, but we have to store that file in a directory called `/tmp`.  So to accomplish this, we pass the variable of `\"/tmp\"` to our `build_files_dir_names` function (see the middle of the `index.py` file).  \n",
    "\n",
    "You can see the result of this by just running the `build_files_dir_names` function:\n",
    "\n",
    "<img src=\"./files-dirs.png\" width=\"100%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e499a3-0d1f-43eb-891b-ea2fa652696e",
   "metadata": {},
   "source": [
    "So you can see above the directory where the file will be stored with `full_dir_path`, and the full file path to the local file with `path_prefix_file_name`.\n",
    "\n",
    "Ok, now before moving this code over to our lambda function, let's make sure that our code works locally.  To do this, set the `bucket_name` to the correct variable, and we can uncomment the last line of the `index.py` file:\n",
    "\n",
    "```python\n",
    "# bucket_data = lambda_handler({}, {})\n",
    "```\n",
    "\n",
    "And then just run `python3 index.py`.\n",
    "\n",
    "From there, confirm that the file is in your s3 bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ca9e33-9065-44b6-a69f-1b26906897ec",
   "metadata": {},
   "source": [
    "> So above, it's in the proper folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa5c646-042a-4d32-b195-d521d815408b",
   "metadata": {},
   "source": [
    "<img src=\"./receipts-file.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f52874-ccc5-4109-86c3-3374cc5aed19",
   "metadata": {},
   "source": [
    "### Migrating to lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661eb187-7fca-4929-b3e4-c17f18ae031a",
   "metadata": {},
   "source": [
    "Ok, so now it's time for you to create a new lambda function, and move the code over there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3387e3fa-0be8-4312-bb41-fbcf2799a25d",
   "metadata": {},
   "source": [
    "<img src=\"./migrated-code.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c25252b-e8af-4091-b162-9b109d908739",
   "metadata": {},
   "source": [
    "> Notice that we created files for `s3_client.py` and `drinks_api_client.py`, and copied the respective code into there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336e4d23-55ae-47d6-bb09-9ce72854d1f4",
   "metadata": {},
   "source": [
    "Then create a `Test` event and confirm that calling the lambda function results in a new json file being uploaded to the s3 bucket.  (Unfortunately, it won't)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d37d72-1011-4aa5-a8c3-d8e6ef6af136",
   "metadata": {},
   "source": [
    "<img src=\"./access-denied.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1561e37-89c7-49f3-a016-1ac33796a960",
   "metadata": {},
   "source": [
    "Take a look at the error message, it says that when trying to upload the file, `when calling the PutObject operation: Access Denied`.\n",
    "\n",
    "Ok, sounds like a permission problem.  So the issue is that our lambda function needs permission to access s3.  Let's take care of that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965dfddd-84e6-4eb3-a98b-9dca2e2286d7",
   "metadata": {},
   "source": [
    "### Setting our S3 permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f379b5-5470-4c26-b0f8-c6b0fa59e39d",
   "metadata": {},
   "source": [
    "Ok, so the best way to set the permissions is to consult [the documentation](https://docs.aws.amazon.com/lambda/latest/dg/with-s3-example.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6350e99a-408c-4d0c-bc65-fdb1a10310a2",
   "metadata": {},
   "source": [
    "Skip down the permissions policy section.  Move through the steps of opening the policies page, and creating a policy, and then pasting the policy into inline json.  (To do this click on the Json tab on the top right).\n",
    "\n",
    "Then paste in the following.\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"logs:PutLogEvents\",\n",
    "                \"logs:CreateLogGroup\",\n",
    "                \"logs:CreateLogStream\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:logs:*:*:*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:PutObject\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:s3:::*/*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9638e387-8c24-4063-86e6-d73b72b87c3c",
   "metadata": {},
   "source": [
    "> We updated this from the documentation to not only grant the `s3:GetObject` permission, but also the `s3:PutObject` permission.\n",
    "\n",
    "Before moving on, make sure we understand the permissions.  There is a separate dictionary for each policy we are granting access to -- the first is cloudwatch logs -- and the second is s3.  Then we are allowing the `Action`s of `GetObject` and `PutObect`.  For the resource, we are specifying access to any s3 bucket, but could be more specific.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8277d364-1b9a-43c5-9268-833557fe9cc9",
   "metadata": {},
   "source": [
    "From there, after clicking `next`, we can confirm that we have both `Read` and `Write` permissions on s3, and write permissions to cloudwatch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34430c98-e8f1-4e54-a1e8-934d880963ba",
   "metadata": {},
   "source": [
    "<img src=\"./confirm-permissions.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40089c39-9363-4744-9401-3e026b61abc8",
   "metadata": {},
   "source": [
    "Then move through the steps of `creating an execution role` that you see in the documentation.\n",
    "\n",
    "So here, you should see that the Principal -- which is what is given the permissions -- is Lambda, and below that these permissions are specified in the attached Policy that we just created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddbb47d-9029-40fd-b572-d751125cfc03",
   "metadata": {},
   "source": [
    "<img src=\"./select-trusted.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cfcde3-6fd1-435a-8ea9-eae628c84ef6",
   "metadata": {},
   "source": [
    "So then we can click on create role.\n",
    "\n",
    "Now that we have created a role, with a policy that has the proper permissions, the next step is to attach this role to our lambda function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd10f38-80ce-4f5d-89e7-1892493281ed",
   "metadata": {},
   "source": [
    "So go to the lambda function, and click on the `configuration` tab, go to the `Execution role` at the top.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7ee566-2f80-4b60-bb68-1eb522bdb076",
   "metadata": {},
   "source": [
    "<img src=\"./edit-execution-role.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d617998-4298-4d34-b5ee-e71ed046f01c",
   "metadata": {},
   "source": [
    "And click on `Edit`, make sure `Use an existing role` is set, and then select the role you just created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69057e0-bf83-447a-b95b-c5b711520d90",
   "metadata": {},
   "source": [
    "<img src=\"./use-existing-role.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f98ebf9-eee9-4581-b886-24928a6f266d",
   "metadata": {},
   "source": [
    "Click save and confirm that the execution role has been updated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f7466b-2f86-411f-bdf6-39d2e7550d42",
   "metadata": {},
   "source": [
    "Ok, so now it's time to check if we can upload a json file to s3.  Test your lambda function again.  This time, we should see the file that was created outputted.  And we should also be able to go to our s3 bucket where the file was uploaded to.  \n",
    "\n",
    "> If there is an error, see if the error occurs locally to identify if it is a problem with your code, or with something realted to working in your lambda function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0eaddd-c20f-4b59-99e7-2c8204dc4a59",
   "metadata": {},
   "source": [
    "### Setting up eventbridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25f1b8e-1cde-48d1-81be-7469dd49cc90",
   "metadata": {},
   "source": [
    "Ok, now that we have our lamba function working by calling it with a test event.  Let's use the eventbridge scheduler so that we make a request to the API every minute, and also write a new file to our s3 bucket every minute.  \n",
    "\n",
    "Remember that to accomplish this, we'll need to move through a few different steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c690739f-3eed-405f-8841-4b004821fa70",
   "metadata": {},
   "source": [
    "First we need to create our cloudwatch rule:\n",
    "\n",
    "```bash\n",
    "aws events put-rule --name run-each-minute --schedule-expression 'rate(1 minute)'\n",
    "```\n",
    "And then added a lambda permission to accept events created by the rule.\n",
    "```bash\n",
    "aws lambda add-permission \\\n",
    "--function-name testfunction \\\n",
    "--action 'lambda:InvokeFunction' \\\n",
    "--statement-id scheduled-event \\\n",
    "--principal events.amazonaws.com \\\n",
    "--source-arn arn:aws:events:us-east-1:086729879076:rule/run-each-minute\n",
    "```\n",
    "\n",
    "Finally, we create a `targets.json` file:\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"Id\": \"1\", \n",
    "    \"Arn\": \"arn:aws:lambda:us-east-1:086729879076:function:testfunction\",\n",
    "    \"Input\": \"{\\\"key\\\":\\\"value\\\"}\"\n",
    "  }\n",
    "]\n",
    "```\n",
    "\n",
    "And then used it to set our lambda function as a target of the rule:\n",
    "\n",
    "```bash\n",
    "aws events put-targets --rule run-each-minute --targets file://targets.json\n",
    "```\n",
    "\n",
    "From here, we can confirm that our eventbridge rule is targetting our lambda function, and our lambda is receiving events from the rule, which is leading it to call the api and save a new file to s3 each minute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf02a21-01b9-4f94-9a29-cfd80680bf50",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Ok, that's it.  After taking a break, review the steps that you went through above to setting up this pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
